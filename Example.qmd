---
title: "Example"
author: "Gregor Steiner"
date: last-modified
editor: visual
theme: cosmo
toc: true  
number-sections: true
colorlinks: true
highlight-style: pygments
format:
  html: 
    code-fold: true
    code-tools: true
    html-math-method: katex
    self-contained: true
bibliography: references.bib
---

# Simulate Data

First, we simulate data based on a simple example with a binary treatment $X$. The outcome $Y$ given an intervention on $X$ is distributed normally, that is 
$$Y | \text{do}(X = x) \sim N(\beta x, \sigma^2),$$
where we set $\beta = 1$. The confounder $Z$ also follows a normal distribution with parameters $\mu_Z = 1$ and $\sigma_Z^2 = 1$. 

```{r message=FALSE, warning=FALSE}
library(causl)

forms <- list(Z ~ 1, 
              X ~ Z,
              Y ~ X, 
              ~ X)

pars <- list(Z = list(beta = 1, phi = 1),
             X = list(beta = c(0, 1), phi = 1),
             Y = list(beta = c(0, 1), phi = 1),
             cop = list(beta = c(0, 1)))

set.seed(12)
n <- 1e3
dat <- rfrugalParam(n, formulas = forms, pars = pars, 
                    family = list(1, 5, 1, 1), careful = FALSE)


```




# Maximum Likelihood

We perform maximum likelihood estimation as described in @evans_didelez_2023 . The observational likelihood is given by
\begin{align*}
	p_{ZXY}(z, x, y | \theta^*) &= p_{ZX}(z, x | \theta_{ZX}) p_{Y|ZX}^*(y|z, x; \theta_{Y|X}^*, \phi_{YZ | X}^*) \\
	&= p_{ZX}(z, x | \theta_{ZX}) p_{Y|X}^*(y | x; \theta_{Y|X}^*) c(y, z| x; \phi_{YZ | X}^*),
\end{align*}
where $c(y, z| x; \phi_{YZ | X}^*)$ is a copula density. By substituting $p_{ZX}$ for the causal distribution $p_{ZX}^*$, where $Z$ and $X$ are independent, we obtain the causal likelihood. Now, we can maximise the causal likelihood w.r.t. the observational data to obtain an estimate for $\beta$. This is done below using the implementation by Robin Evans.


```{r}
fit <- fitCausal(dat, formulas = list(Y ~ X, Z ~ 1, ~ X),
                 family = c(1, 1, 1), control=list(maxit=2e4, newton=TRUE))
fit
```

The estimate is close to the true value of 1. For comparison, we can also perform a naive outcome regression, where we obtain a confidence interval for $\beta_X$ excluding the true value of 1.

```{r}
lm(Y ~ X, data = dat) |> confint()
```



# Bayesian Approach

Now, we try to fit this in a Bayesian fashion. We implement a simple Metropolis algorithm using a diffuse prior, and a (truncated) multivariate normal jump distribution. I manually implemented a log-likelihood function, using a Gaussian copula with correlation parameter $\phi$. I think $\phi$ should depend on $X$, but I am a bit unsure on how to correctly implement this.


```{r}
likl <- function(theta){
  beta <- theta[1]; sigma <- theta[2]
  beta_z <- theta[3]; sigma_z <- theta[4]
  phi <- theta[5]
  
  with(dat, {
    # marginal structural model
    p_Y_X <- dnorm(Y, beta * X, sigma, log = TRUE) |> sum()
    
    # past
    p_ZX <- dnorm(Z, beta_z, sigma_z, log = TRUE) |> sum() +
      dnorm(X, log = TRUE) |> sum()
    
    # copula
    cop <- copula::normalCopula(param = c(phi))
    u <- matrix(NA, nrow = nrow(dat), ncol = 2)
    u[, 1] <- pnorm(Y, beta * X, sigma)
    u[, 2] <- pnorm(Z, beta_z, sigma_z)
    copd <- copula::dCopula(cop, u = u, log = TRUE) |> sum()
    
    # return sum (since we did all calculations on a log-scale)
    return(p_Y_X + p_ZX + copd)
    })
}



```

```{r}
library(mvtnorm)

# prior parameters
p <- 5
sigma_p <- diag(rep(10, p))
mu_p <- c(1, 1, 1, 1, 0)
prior <- function(theta) 1 / (theta[2] * theta[4])

# jump distribution parameters
sigma_j <- diag(c(0.001, 0.001, 0.001, 0.001, 0.001))

# initial values
theta_0 <- c(0.5, 1.2, 1.5, 0.8, 0)

# storage
m <- 5e3
warm <- 1e3
store <- matrix(NA, nrow = m, ncol = p)
colnames(store) <- c("beta", "sigma", "beta_Z", "sigma_Z", "phi")
store[1, ] <- theta_0

set.seed(12)
for (i in 2:m) {
  curr <- store[i-1, ]
  prop <- rmvnorm(1, curr, sigma_j)
  prop[c(2, 4)] <- ifelse(prop[c(2, 4)] <= 0, 1, prop[c(2, 4)])
  prop[5] <- ifelse(prop[5] < -1 | prop[5] > 1, 0, prop[5])
  
  r <- (likl(prop) + prior(prop)) - (likl(curr) + prior(curr))
  
  if(runif(1) < min(exp(r), 1)){
    store[i, ] <- prop
  } else{
    store[i, ] <- curr
  }
  
}



```



```{r}
par(mfrow = c(1, 2), mar = c(4, 4, 1, 1))
hist(store[warm:m, 1], breaks = 40, freq = FALSE, 
     main = "", xlab = "Beta")
plot(store[warm:m, 1], type = "l",
     main = "",
     ylab = "Beta", xlab = "")
```
Nevertheless, we get a posterior distribution for $\beta$ which is nicely centered around the MLE from above.



