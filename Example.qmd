---
title: "Bayesian Inference with a frugal parameterization"
author: "Gregor Steiner"
date: last-modified
editor: visual
theme: cosmo
toc: true  
number-sections: true
colorlinks: true
highlight-style: pygments
format:
  html: 
    code-fold: true
    code-tools: true
    html-math-method: katex
    self-contained: true
bibliography: references.bib
---

# Simulate Data

This notebook works through a simple example using the frugal parameterization proposed by @evans_didelez_2023 . First, we simulate data based on a simple model with an outcome $Y$, a binary treatment $X$, and a confounder $Z$. The setup is $$ Z \sim N(\mu_Z, \sigma_Z^2) \\
X | Z = z \sim Ber( \text{expit}(z)) \\
Y | \text{do}(X = x) \sim N(\beta x, \sigma^2).$$

The dependence between $Y$ and $Z$ is modelled using a Gaussian copula with correlation parameter $\phi_{YZ|X}^* = 2 \text{expit}(\alpha_{\phi} + \beta_{\phi} X) - 1$. Thus, the frugal parameterization consists of $\theta_{ZX} = (\mu_Z, \sigma_Z)$, $\theta_{Y|X}^* = (\beta, \sigma^2)$, and $\phi_{YZ|X}^*$, the correlation parameter of $Y$ and $Z$ conditional on $X$. The data is simulated with $\beta = 1$, $\sigma^2 = 1$, $\mu_Z = 1/2$, $\sigma_Z^2 = 4$, $\alpha_{\phi} = 0$, and $\beta_{\phi} = 1$.

```{r message=FALSE, warning=FALSE}
#devtools::install_github("rje42/causl")
library(causl)

forms <- list(Z ~ 1, 
              X ~ Z,
              Y ~ X, 
              ~ X)

pars <- list(Z = list(beta = 1/2, phi = 4),
             X = list(beta = c(0, 1)),
             Y = list(beta = c(0, 1), phi = 1),
             cop = list(beta = c(0, 1)))

set.seed(12)
n <- 1e3
dat <- rfrugalParam(n, formulas = forms, pars = pars, 
                    family = list(1, 5, 1, 1), careful = FALSE)


```

# Maximum Likelihood

We perform maximum likelihood estimation as described in @evans_didelez_2023 . The observational likelihood is given by \begin{align*}
    p_{ZXY}(z, x, y | \theta^*) &= p_{ZX}(z, x | \theta_{ZX}) p_{Y|ZX}^*(y|z, x; \theta_{Y|X}^*, \phi_{YZ | X}^*) \\
    &= p_{ZX}(z, x | \theta_{ZX}) p_{Y|X}^*(y | x; \theta_{Y|X}^*) c(y, z| x; \phi_{YZ | X}^*),
\end{align*} where $c(y, z| x; \phi_{YZ | X}^*)$ is a copula density. By substituting $p_{ZX}$ for the causal distribution $p_{ZX}^*$, where $Z$ and $X$ are independent, we obtain the causal likelihood. Now, we can maximise the causal likelihood w.r.t. the observational data to obtain an estimate for $\beta$. This is done below using the implementation by Robin Evans.

```{r}
fit <- fitCausal(dat, formulas = list(Y ~ X, Z ~ 1, ~ X),
                 family = c(1, 1, 1), control=list(maxit=2e4, newton=TRUE))
fit
```

The estimate is close to the true value of 1. For comparison, we can also perform a naive outcome regression, where we obtain a confidence interval for $\beta_X$ excluding the true value of 1.

```{r}
lm(Y ~ X, data = dat) |> confint()
```

# Bayesian Approach

## Plug-In

Now, we try to fit this in a Bayesian fashion. We implement a simple Metropolis algorithm using a (truncated) multivariate normal jump distribution. I manually implemented a log-likelihood function. Since $X$ is binary I will simply take the correlation within each group as a plug-in estimate for the correlation parameter. This plug-in approach is obviously not fully Bayesian (see further below for a fully Bayesian approach). As prior distributions I use normal priors for the location parameters and exponential priors for the scale parameters.

```{r}
likl <- function(theta){
  beta <- theta[1]; sigma <- theta[2]
  beta_z <- theta[3]; sigma_z <- theta[4]
  phi_1 <- theta[5]; phi_0 <- theta[6]
  
  with(dat, {
    # marginal structural model
    p_Y_X <- dnorm(Y, beta * X, sigma, log = TRUE) |> sum()
    
    # past
    p_ZX <- dnorm(Z, beta_z, sigma_z, log = TRUE) |> sum() +
      dbinom(X, size = 1, prob = mean(X), log = TRUE) |> sum() # not sure about this line
    # is this really the causal distribution of X?
    
    # get copula density separately by group
    bool <- X == 1
    cop_1 <- copula::normalCopula(param = phi_1)
    u_1 <- matrix(NA, nrow = sum(bool), ncol = 2)
    u_1[, 1] <- pnorm(Y[bool], beta * X[bool], sigma)
    u_1[, 2] <- pnorm(Z[bool], beta_z, sigma_z)
    copd_1 <- copula::dCopula(cop_1, u = u_1, log = TRUE) |> sum()
    
    cop_0 <- copula::normalCopula(param = phi_0)
    u_0 <- matrix(NA, nrow = sum(!bool), ncol = 2)
    u_0[, 1] <- pnorm(Y[!bool], beta * X[!bool], sigma)
    u_0[, 2] <- pnorm(Z[!bool], beta_z, sigma_z)
    copd_0 <- copula::dCopula(cop_0, u = u_0, log = TRUE) |> sum()
    
    copd <- copd_0 + copd_1
    
    # return sum (since we did all calculations on a log-scale)
    return(p_Y_X + p_ZX + copd)
    })
}

```

```{r}
library(mvtnorm)

# get plug in estimate for phi
phi_1 <- with(dat, cor(Y[X == 1], Z[X == 1]))
phi_0 <- with(dat, cor(Y[X == 0], Z[X == 0]))

# prior
p = 4
prior <- function(theta){
  dnorm(theta[1], 0, 10, log = TRUE) + # beta
    dexp(theta[2], 1/10, log = TRUE) + # sigma
    dnorm(theta[3], 0, 10, log = TRUE) + # mu_Z
    dexp(theta[4], 1/10, log = TRUE) # sigma_Z
}

# jump distribution parameters
sigma_j <- diag(c(0.001, 0.001, 0.001, 0.001))

# initial values
theta_0 <- c(0.5, 1.2, 0.5, 0.8)

# storage
m <- 5e3
warm <- 1e3
store <- matrix(NA, nrow = m, ncol = p)
colnames(store) <- c("beta", "sigma", "beta_Z", "sigma_Z")
store[1, ] <- theta_0

set.seed(12)
for (i in 2:m) {
  curr <- store[i-1, ]
  prop <- rmvnorm(1, curr, sigma_j)
  prop[c(2, 4)] <- ifelse(prop[c(2, 4)] <= 0, 1, prop[c(2, 4)])
  
  r <- (likl(c(prop, phi_1, phi_0)) + prior(prop)) - (likl(c(curr, phi_1, phi_0)) + prior(curr))
  
  if(runif(1) < min(exp(r), 1)){
    store[i, ] <- prop
  } else{
    store[i, ] <- curr
  }
  
}



```

```{r}
par(mfrow = c(1, 2), mar = c(4, 4, 1, 1))
hist(store[warm:m, 1], breaks = 40, freq = FALSE, 
     main = "", xlab = "Beta")
plot(store[warm:m, 1], type = "l",
     main = "",
     ylab = "Beta", xlab = "")
```

The posterior distribution of beta is nicely centered around the true value of 1.

## Fully Bayesian

Here, we try a fully Bayesian approach, that is we do not plug in an estimate for $\phi$, but include it in the parameter vector and also obtain a posterior distribution. However, it is important to keep the dependence on the group, so we sample separately for $\phi_1$ and $\phi_0$, where $\phi_i = \text{Cor}(Y, Z | X = i)$. As prior distributions for the correlation parameters we simply use a uniform on $(-1, 1)$.

```{r}
# prior
p = 6
prior <- function(theta){
  dnorm(theta[1], 0, 10, log = TRUE) + # beta
    dexp(theta[2], 1/10, log = TRUE) + # sigma
    dnorm(theta[3], 0, 10, log = TRUE) + # mu_Z
    dexp(theta[4], 1/10, log = TRUE) + # sigma_Z
    dunif(theta[5], -1, 1, log = TRUE) + # phi_1
    dunif(theta[6], -1, 1, log = TRUE) # phi_0
}

# jump distribution parameters
sigma_j <- diag(c(0.001, 0.001, 0.001, 0.001, 0.001, 0.001))

# initial values
theta_0 <- c(0.5, 1.2, 1.5, 0.8, 0, 0)

# storage
m <- 5e3
warm <- 1e3
store <- matrix(NA, nrow = m, ncol = p)
colnames(store) <- c("beta", "sigma", "beta_Z", "sigma_Z", "phi_1", "phi_0")
store[1, ] <- theta_0

set.seed(12)
for (i in 2:m) {
  curr <- store[i-1, ]
  prop <- rmvnorm(1, curr, sigma_j)
  prop[c(2, 4)] <- ifelse(prop[c(2, 4)] <= 0, 1, prop[c(2, 4)])
  prop[5] <- ifelse(prop[5] < -1 | prop[5] > 1, 0, prop[5])
  prop[6] <- ifelse(prop[6] < -1 | prop[6] > 1, 0, prop[6])
  
  r <- (likl(prop) + prior(prop)) - (likl(curr) + prior(curr))
  
  if(runif(1) < min(exp(r), 1)){
    store[i, ] <- prop
  } else{
    store[i, ] <- curr
  }
  
}

```

```{r}
par(mfrow = c(1, 2), mar = c(4, 4, 1, 1))
hist(store[warm:m, 1], breaks = 40, freq = FALSE, 
     main = "", xlab = "Beta")
plot(store[warm:m, 1], type = "l",
     main = "",
     ylab = "Beta", xlab = "")
```

Again the posterior distribution of $\beta$ is centered around the true value of 1.

```{r}
par(mfrow = c(1, 2), mar = c(4, 4, 1, 1))
hist(store[warm:m, 5], breaks = 30, freq = FALSE, 
     main = "", xlab = "Phi_1")
hist(store[warm:m, 6], breaks = 30, freq = FALSE, 
     main = "", xlab = "Phi_0")
```

The posterior distributions for $\phi_1$ and $\phi_0$ are perhaps slightly off with the true values of $\approx0.46$ and $0$ only being in the tails.

# 
